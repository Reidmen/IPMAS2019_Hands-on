{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "    <img align=\"left\" src=\"img/DockerLogo.png\" width=\"300\"/> <img align=\"center\" src=\"img/FenicsLogo.png\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why virtualization using Docker?\n",
    "\n",
    "<p align=\"left\">\n",
    "    <ul>\n",
    "        <li> Portable deployment of applications as a single object. </li>\n",
    "        <li> Application-centric paradigm. </li>\n",
    "        <li> Reusable components. </li>\n",
    "        <li> Public registry for sharing containers.</li>\n",
    "        <li> A Growing Ecosystem since their release, 2014. </li>\n",
    "     </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "    <img align=\"left\" src=\"img/DockerArxivFile.png\" width=\"400\"/> <img align=\"center\" src=\"img/DockerArchitecture.png\" width=\"400\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use FEniCS?\n",
    "* An open-source software (C++, Python) with specialized libraries for numerical computation.\n",
    "* Allows ``automatic`` solution of partial differential equations using the FEM theory.\n",
    "* It contains libraries such as UFL, FIAT, FFC, DOLFIN, etc.\n",
    "\n",
    "### Repositories:\n",
    "* **https://fenicsproject.org/**\n",
    "* **https://github.com/FEniCS**\n",
    "* **https://bitbucket.org/fenics-project/dolfin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Jupyter?\n",
    "* A new form of prototyping problem and present projects.\n",
    "* On a interactive fashion.\n",
    "* Lots of features, a combination of frameworks.\n",
    "* Much more!..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "- Installation of hIPPYlib on the terminal\n",
    "- Undestand FEniCS usage\n",
    "- Solve the forward and adjoint Poisson equations\n",
    "- Understand the inverse method framework\n",
    "- Visualise results\n",
    "- Modify the code!\n",
    "\n",
    "## Tools\n",
    "\n",
    "- Finite element method\n",
    "- Derivation of gradient via the adjoint method\n",
    "- Armijo line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coefficient field inversion in an elliptic partial differential equation\n",
    "\n",
    "We consider the estimation of a coefficient in an elliptic partial\n",
    "differential equation (PDE) as a model problem. \n",
    "Depending of the interpretation, it defines:\n",
    "\n",
    "* The inversion for groundwater flow or heat conductivity.  \n",
    "* The seach of membrane with a certain spatially varying stiffness. \n",
    "\n",
    "Let $\\Omega\\subset\\mathbb{R}^n$, $n\\in\\{1,2,3\\}$ be an open, bounded\n",
    "domain and consider the following problem:\n",
    "\n",
    "$$\n",
    "\\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx,\n",
    "$$\n",
    "\n",
    "where $u$ is the solution of\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\quad -\\nabla\\cdot(e^m \\nabla u) &= f \\text{ in }\\Omega,\\\\\n",
    "u &= 0 \\text{ on }\\partial\\Omega.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Here $m \\in \\mathcal{M}:=\\{m\\in L^{\\infty}(\\Omega) \\bigcap H^1(\\Omega)\\}$ denotes the unknown coefficient field, \n",
    "$u \\in H^1_0(\\Omega)$ the state variable, $u_d$ the (possibly noisy) data, $f\\in H^{-1}(\\Omega)$ a given volume force, and $\\gamma\\ge 0$ the regularization parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The variational (or weak) form of the state equation:\n",
    "\n",
    "Find $u\\in H_0^1(\\Omega)$ such that \n",
    "\n",
    "$$(e^m \\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega),$$\n",
    "\n",
    "where $H_0^1(\\Omega)$ is the space of functions vanishing on $\\partial\\Omega$ with square integrable derivatives. \n",
    "\n",
    "Above, $(\\cdot\\,\\cdot)$ denotes the $L^2$-inner product, i.e, for scalar functions $u,v$ defined on $\\Omega$ we write \n",
    "\n",
    "$$(u,v) := \\int_\\Omega u(x) v(x) \\,dx, $$\n",
    "\n",
    "and similarly for vector functions $\\boldsymbol{u}, \\boldsymbol{v}$ defined on $\\Omega$ we write\n",
    "\n",
    "$$(\\boldsymbol{u},\\boldsymbol{v}) := \\int_\\Omega \\boldsymbol{u}(x) \\cdot \\boldsymbol{v}(x) \\,dx.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient evaluation:\n",
    "\n",
    "The Lagrangian functional $\\mathscr{L}:H_0^1(\\Omega)\\times\\mathcal{M}\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n",
    "\\frac{\\gamma}{2}(\\nabla m, \\nabla m) +  (e^m\\nabla u,\\nabla p) - (f,p).\n",
    "$$\n",
    "\n",
    "Then the gradient of the cost functional $\\mathcal{J}(m)$ with respect to the parameter $m$ is\n",
    "\n",
    "$$\n",
    "    \\mathcal{G}(m)(\\tilde m) := \\mathscr{L}_m(u,m,p)(\\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +\n",
    "     (\\tilde{m}e^m\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in \\mathcal{M},\n",
    "$$\n",
    "\n",
    "where $u \\in H_0^1(\\Omega)$ is the solution of the forward problem,\n",
    "\n",
    "$$ \\mathscr{L}_p(u,m,p)(\\tilde{p})  := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0\n",
    "\\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), $$\n",
    "\n",
    "and $p \\in H_0^1(\\Omega)$ is the solution of the adjoint problem,\n",
    "\n",
    "$$ \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0\n",
    "\\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Steepest descent method.\n",
    "\n",
    "* The steepest descent methods computes an update direction $\\hat{m}_k$ in the direction of the negative gradient defined as \n",
    "\n",
    "$$\n",
    "(\\hat{m}_k, \\tilde{m} ) = -\\mathcal{G}(m_k)(\\tilde m) := -\\mathscr{L}_{m}(u_k,m_k,p_k)(\\tilde{m}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),\n",
    "$$\n",
    "\n",
    "  where the evaluation of the gradient $\\mathcal{G}(m_k)$ involve the solution $u_k$ and $p_k$ of the forward and adjoint problem (respectively) for $m = m_k$.\n",
    "\n",
    "* Then its set the update $m_{k+1} = m_k + \\alpha \\hat{m}_k$, where the step length $\\alpha$ is chosen to guarantee sufficient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Dolfin --> FEniCS library and hPPYlib library\n",
    "import dolfin as dl\n",
    "from hippylib import nb\n",
    "\n",
    "# The useful numpy package for numerical computation\n",
    "import numpy as np\n",
    "# logging just for debugging purposes\n",
    "import logging\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model set up:\n",
    "\n",
    "As in the introduction, the first thing we need to do is to set up the numerical model.\n",
    "\n",
    "In this cell, we set the mesh ``mesh``, the finite element spaces ``Vm`` and ``Vu`` corresponding to the parameter space and state/adjoint space, respectively. In particular, we use linear finite elements for the parameter space, and quadratic elements for the state/adjoint space.\n",
    "\n",
    "#### How powerfull is FEniCS to define meshes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mshr package\n",
    "import mshr as mh\n",
    "import math as mt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import HTML\n",
    "#HTML(dl.X3DOM().html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true parameter ``mtrue`` is the finite element interpolant of the function\n",
    "\n",
    "$$ m_{\\rm true} = \\left\\{ \\begin{array}{l} \\ln 4 \\; \\forall \\,(x,y)  \\in \\, \\mathbf{B}_{0.15}((0.3,0.3)) \\cup \\mathbf{B}_{0.15}((0.7,0.7)) \\\\ \\ln 8 \\; {\\rm otherwise}. \\end{array}\\right. $$\n",
    "\n",
    "The forcing term ``f`` and the boundary conditions ``u0`` for the forward problem are\n",
    "\n",
    "$$ f = 1 \\; \\forall {\\bf x} \\in \\Omega, \\quad u = 0 \\; \\forall {\\bf x} \\in \\partial \\Omega. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The FEniCS paradigm uses the definition of mesh\n",
    "# and then the definition of Function Spaces for the weak formulations\n",
    "# Create mesh and define function spaces\n",
    "nx = 32\n",
    "ny = 32\n",
    "#mesh = dl.UnitSquareMesh(nx, ny)\n",
    "#Vm = \n",
    "#Vu = \n",
    "\n",
    "# The true and initial guess for inverted parameter\n",
    "# defined using c++ expressions\n",
    "\n",
    "m_true = dl.interpolate(dl.Expression(cpp, degree=2), Vm)\n",
    "\n",
    "# Define function for state and adjoint\n",
    "\n",
    "# Define Trial and Test Functions\n",
    "\n",
    "# Initialize input functions\n",
    "f  = dl.Constant(1.0)\n",
    "u0 = dl.Constant(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "#plt.figure(figsize=(15,5))\n",
    "#nb.plot(mesh, subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\n",
    "#nb.plot(m_true, subplot_loc=122, mytitle=\"True parameter field\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up dirichlet boundary conditions\n",
    "# They can be defined using Python subclasses, or by cpp code\n",
    "cpp_bc = 'on_boundary'\n",
    "#bc_state = \n",
    "#bc_adj   = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Set up synthetic observations:\n",
    "\n",
    "To generate the synthetic observation we first solve the PDE for the state variable ``utrue`` corresponding to the true parameter ``mtrue``.\n",
    "Specifically, we solve the variational problem:\n",
    "\n",
    "Find $u\\in \\mathcal{V}$ such that \n",
    "\n",
    "$$\\underbrace{(e^{m_{\\text true}} \\nabla u,\\nabla v)}_{\\; := \\; a_{\\rm true}} - \\underbrace{(f,v)}_{\\; := \\;L_{\\rm true}} = 0, \\text{ for all } v\\in \\mathcal{V}$$\n",
    "\n",
    "Then we perturb the true state variable and write the observation ``ud`` as\n",
    "\n",
    "$$ u_{d} = u_{\\rm true} + \\eta, \\quad {\\rm where} \\; \\eta \\sim \\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "Here the standard variation $\\sigma$ is proportional to ``noise_level``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Noise level, we fixed to 5%\n",
    "noise_level = 0.05\n",
    "\n",
    "# Weak form for setting up the synthetic observations\n",
    "#a_true = \n",
    "#L_true = \n",
    "\n",
    "# Solve the forward/state problem to generate synthetic observations\n",
    "#A_true, b_true = \n",
    "\n",
    "u_true = dl.Function(Vu)\n",
    "dl.solve(A_true, u_true.vector(), b_true)\n",
    "\n",
    "# Assign u_true solution to the ud observation variable\n",
    "ud = dl.Function(Vu)\n",
    "ud.assign(u_true)\n",
    "\n",
    "# Perturb state solution and create synthetic measurements ud\n",
    "# ud = u + ||u||* random.normal\n",
    "#MAX = \n",
    "# Initialize \"noise\" vector\n",
    "#noise = \n",
    "# Create noise vector with dimenstion compatible to solve y = Ax (\"1\" to be the \"x\")\n",
    "#A_true.init_vector(noise, 1)\n",
    "#noise.set_local( noise_level * MAX * np.random.normal(0, 1, len(ud.vector().get_local())) )\n",
    "# Apply the boundary conditions bc_adj to the noise vector\n",
    "#bc_adj.apply(noise)\n",
    "\n",
    "# Update the ud vector --> ud += 1*noise\n",
    "#ud.vector().axpy(1., noise)\n",
    "\n",
    "# Plot\n",
    "#nb.multi1_plot([u_true, ud], [\"State Solution with mtrue\", \"Synthetic Observations\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost functional evaluation:\n",
    "We consider the cost functional with a Tikhonov regularization:\n",
    "$$\n",
    "J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text reg}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameter\n",
    "gamma = 1e-9\n",
    "\n",
    "# Define cost function\n",
    "def cost(u, ud, m, gamma):\n",
    "    \"We'll return a list of [reg + misfit, misfit, reg]\"\n",
    "    #reg =\n",
    "    #misfit = \n",
    "    return [reg + misfit, misfit, reg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the variational form for the state/adjoint equations and gradient evaluation\n",
    "\n",
    "Below its will be defined the variational forms that appears in the the state/adjoint equations and gradient evaluations.\n",
    "\n",
    "Specifically,\n",
    "\n",
    "- `a_state`, `L_state` stand for the bilinear and linear form of the state equation, repectively;\n",
    "- `a_adj`, `L_adj` stand for the bilinear and linear form of the adjoint equation, repectively;\n",
    "- `CTvarf`, `gradRvarf` stand for the contributions to the gradient coming from the PDE and the regularization, respectively.\n",
    "\n",
    "We also build the *mass* matrix $M$ that is used to discretize the $L^2(\\Omega)$ inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak form for setting up the state equation\n",
    "#a_state = \n",
    "#L_state = \n",
    "\n",
    "# Weak form for setting up the adjoint equations\n",
    "#a_adj = \n",
    "#L_adj = \n",
    "\n",
    "# Weak form for gradient\n",
    "#CTvarf    = \n",
    "#gradRvarf = \n",
    "\n",
    "# Mass matrix in parameter space\n",
    "#Mvarf = \n",
    "#M = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite difference check of the gradient\n",
    "\n",
    "We use a **finite difference check** to verify that our gradient derivation is correct.\n",
    "Specifically, we consider a function $ m_0\\in \\mathcal{M}$ and we verify that for an arbitrary direction $\\tilde{m} \\in \\mathcal{M}$ we have\n",
    "$$ r := \\left| \\frac{ \\mathcal{J}(m_0 + \\varepsilon \\tilde{m}) - \\mathcal{J}(m_0)}{\\varepsilon} -  \\mathcal{G}(m_0)(\\tilde{m})\\right| = \\mathcal{O}(\\varepsilon).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, interpolate the m_0 = log(4) to the function space\n",
    "m0 = dl.interpolate(dl.Constant(np.log(4.) ), Vm )\n",
    "\n",
    "# Then, define a set of epsilons to check their errors\n",
    "n_eps = 32\n",
    "eps = np.power(2., -np.arange(n_eps))\n",
    "err_grad = np.zeros(n_eps)\n",
    "\n",
    "m.assign(m0)\n",
    "\n",
    "# To construct J(m_0) we need the forward solution u\n",
    "# Solve the fwd problem and evaluate the cost functional\n",
    "#A, state_b = \n",
    "#dl.solve(A, u.vector(), state_b)\n",
    "\n",
    "c0, _, _ = cost(u, ud, m, gamma)\n",
    "\n",
    "# We also need the adjoint solution p\n",
    "# Solve the adjoint problem and evaluate the gradient\n",
    "adj_A, adjoint_RHS = dl.assemble_system(a_adj, L_adj, bc_adj)\n",
    "dl.solve(adj_A, p.vector(), adjoint_RHS)\n",
    "\n",
    "# Evaluate the  gradient\n",
    "#grad0 = \n",
    "\n",
    "# Define an arbitrary direction m_hat to perform the check \n",
    "mtilde = dl.Function(Vm).vector()\n",
    "mtilde.set_local(np.random.randn(Vm.dim()))\n",
    "mtilde.apply(\"\")\n",
    "mtilde_grad0 = grad0.inner(mtilde)\n",
    "\n",
    "for i in range(n_eps):\n",
    "    # Generate the update m0 --> m0+eps*mtilde\n",
    "\n",
    "    # Solve the direct problem\n",
    "    #A, state_b = \n",
    "    #dl.solve(A, u.vector(), state_b)\n",
    "    \n",
    "    # Obtain the cost functional\n",
    "    cplus, _, _ = cost(u, ud, m, gamma)\n",
    "   \n",
    "    #err_grad[i] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below will show in a loglog scale the value of $r$ as a function of $\\varepsilon$. We observe that $r$ decays linearly for a wide range of values of $\\varepsilon$, however we notice an increase in the error for extremely small values of $\\varepsilon$ due to numerical stability and finite precision arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()    \n",
    "plt.loglog(eps, err_grad, \"-ob\", label=\"Error Grad\")\n",
    "plt.loglog(eps, (.5*err_grad[0]/eps[0])*eps, \"-.k\", label=\"First Order\")\n",
    "plt.title(\"Finite difference check of the first variation (gradient)\")\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"Error grad\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial guess\n",
    "\n",
    "We solve the state equation and compute the cost functional for the initial guess of the parameter ``m0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we assign the m_0 = log 4 to the m Function\n",
    "m.assign(m0)\n",
    "\n",
    "# Solve state equation\n",
    "#A_state, b_state = \n",
    "dl.solve(A_state, u.vector(), b_state)\n",
    "\n",
    "# Evaluate cost\n",
    "[cost_old, misfit_old, reg_old] = cost(u, ud, m, gamma)\n",
    "\n",
    "# Plot using the redefined plots function with the nb library\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(m,subplot_loc=121, mytitle=\"m0\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max())\n",
    "nb.plot(u,subplot_loc=122, mytitle=\"u(m0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steepest descent with Armijo line search:\n",
    "\n",
    "* Its solved the constrained optimization problem using the steepest descent method with Armijo line search.\n",
    "\n",
    "* The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. $\\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau$).\n",
    "\n",
    "* The gradient is computed by solving the state and adjoint equation for the current parameter $m$, and then substituing the current state $u$, parameter $m$ and adjoint $p$ variables in the weak form expression of the gradient: \n",
    "$$ (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}e^m\\nabla u, \\nabla p).$$\n",
    "\n",
    "* The Armijo line search uses backtracking to find $\\alpha$ such that a sufficient reduction in the cost functional is achieved. Specifically, we use backtracking to find $\\alpha$ to guaratee optimal desceent such that:\n",
    "$$J( m - \\alpha g ) \\leq J(m) - \\alpha c_{\\rm armijo} (g,g). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the optimization\n",
    "tol = 1e-4\n",
    "#maxiter = \n",
    "#print_any = \n",
    "#plot_any = \n",
    "c_armijo = 1e-5\n",
    "\n",
    "# Initialize iter counters\n",
    "iter = 0\n",
    "converged = False\n",
    "\n",
    "# Initilize direction vector\n",
    "g = dl.Vector()\n",
    "# and their correct dimensions \n",
    "M.init_vector(g, 0)\n",
    "\n",
    "m_prev = dl.Function(Vm)\n",
    "\n",
    "print( \"N.It Cost          Misfit        Reg         ||grad||       Alpha  N Backtrack\" )\n",
    "\n",
    "# We'll iterate until a maxiter number and if has not converged\n",
    "while iter <  maxiter and not converged:\n",
    "\n",
    "    # Solve the adoint problem\n",
    "    #adj_A, adjoint_RHS = \n",
    "    dl.solve(adj_A, p.vector(), adjoint_RHS)\n",
    "\n",
    "    # Evaluate the  gradient\n",
    "    MG = dl.assemble(CTvarf + gradRvarf)\n",
    "    dl.solve(M, g, MG)\n",
    "\n",
    "    # Calculate the norm of the gradient\n",
    "    grad_norm2 = g.inner(MG)\n",
    "    gradnorm = np.sqrt(grad_norm2)\n",
    "    \n",
    "    # Start the gradnorm\n",
    "    if iter == 0:\n",
    "        gradnorm0 = gradnorm\n",
    "\n",
    "    # Armijo Linesearch\n",
    "    it_backtrack = 0\n",
    "    m_prev.assign(m)\n",
    "    alpha = 1.E5\n",
    "    backtrack_converged = False\n",
    "    for it_backtrack in range(20):\n",
    "        \n",
    "        # Generate vector m - alpha*g\n",
    "        m.vector().axpy(-alpha, g )\n",
    "        \n",
    "        # Obtain the J(m-alpha*g) functional by\n",
    "        # Solving the state/forward problem\n",
    "        #state_A, state_b = \n",
    "        #dl.solve(state_A, u.vector(), state_b)\n",
    "\n",
    "        # Then, evaluating at the cost\n",
    "        [cost_new, misfit_new, reg_new] = cost(u, ud, m, gamma)\n",
    "\n",
    "        # check if Armijo conditions are satisfied\n",
    "        if cost_new < cost_old - alpha * c_armijo * grad_norm2:\n",
    "            cost_old = cost_new\n",
    "            backtrack_converged = True\n",
    "            break\n",
    "        else:\n",
    "            alpha *= 0.5\n",
    "            m.assign(m_prev)  # reset a\n",
    "            \n",
    "    if backtrack_converged == False:\n",
    "        print( \"Backtracking failed. A sufficient descent direction was not found\" )\n",
    "        converged = False\n",
    "        break\n",
    "    \n",
    "    # Print a lot of info!.\n",
    "    sp = \"\"\n",
    "    if (iter % print_any)== 0 :\n",
    "        print( \"%3d %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %3d\" % \\\n",
    "            (iter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n",
    "            gradnorm, sp, alpha, sp, it_backtrack) )\n",
    "\n",
    "    if (iter % plot_any)== 0 :\n",
    "        nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False)\n",
    "        plt.show()\n",
    "    \n",
    "    # check for convergence\n",
    "    if gradnorm < tol*gradnorm0 and iter > 0:\n",
    "        converged = True\n",
    "        print (\"Steepest descent converged in \",iter,\"  iterations\")\n",
    "        \n",
    "    iter += 1\n",
    "    \n",
    "if not converged:\n",
    "    print ( \"Steepest descent did not converge in \", maxiter, \" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, plot the results obtained!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on!\n",
    "\n",
    "### Question 1\n",
    "\n",
    "> Whats the number of steepest descent iterations needed for a discretization of the domain with $16 \\times 16$, $32 \\times 32$, $64 \\times 64$ finite elements? \n",
    "  how the number of iterations changes as the inversion parameter mesh is refined?\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "> Since the coefficient $m$ is discontinuous, a better choice of regularization is total variation rather than Tikhonov regularization, to prevent an overly smooth reconstruction. \n",
    "Modify the implementation and plot the result for a reasonably chosen regularization parameter $\\gamma$, in the form:\n",
    "\n",
    "> $$ \\mathcal{R}_{\\rm TV}^\\varepsilon = \\gamma \\int_\\Omega (\\nabla m \\cdot \\nabla m + \\varepsilon)^{\\frac{1}{2}}\\, dx. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
